# compute/controller.py
import redis
import os
import json
import argparse

# é…ç½®
REDIS_HOST = 'redis'
REDIS_PORT = 6379
DATA_DIR = "/app/data"
INPUT_FILE = os.path.join(DATA_DIR, "intermediate", "corpus.jsonl")


def reset_redis(r):
    # æ¸…ç†æ‰€æœ‰ç›¸å…³é˜Ÿåˆ—
    queues = [
        'queue:indexing:mapper',
        'queue:indexing:mapper:processing', # æ–°å¢
        'queue:indexing:reducer',
        'queue:indexing:reducer:processing' # æ–°å¢
    ]
    r.delete(*queues)
    print("ğŸ§¹ [Indexing Controller] All queues (pending & processing) cleared.")


def publish_mapper_tasks(r, chunk_size=1000):
    """
    æ‰«ææ–‡ä»¶ï¼Œç”Ÿæˆå¸¦æœ‰ã€å­—èŠ‚åç§»é‡ã€‘çš„ä»»åŠ¡
    """
    print(f"ğŸ“¦ Scanning {INPUT_FILE} to generate tasks...")
    if not os.path.exists(INPUT_FILE):
        print("âŒ File not found.")
        return

    tasks = []
    task_id = 0

    with open(INPUT_FILE, 'rb') as f:  # äºŒè¿›åˆ¶æ¨¡å¼è¯»å–ï¼Œä¿è¯åç§»é‡å‡†ç¡®
        start_offset = 0
        lines_count = 0

        for line in f:
            lines_count += 1
            if lines_count >= chunk_size:
                # è®°å½•å½“å‰æŒ‡é’ˆä½ç½®
                end_offset = f.tell()

                # ç”Ÿæˆä»»åŠ¡åŒ…
                task = {
                    "task_id": task_id,
                    "start_offset": start_offset,
                    "read_bytes": end_offset - start_offset  # åªéœ€è¦è¯»è¿™ä¹ˆå¤šå­—èŠ‚
                }
                r.rpush('queue:indexing:mapper', json.dumps(task))

                # é‡ç½®è®¡æ•°å™¨
                task_id += 1
                lines_count = 0
                start_offset = end_offset

        # å¤„ç†å‰©ä½™çš„æœ€åä¸€å—
        if lines_count > 0:
            end_offset = f.tell()
            task = {
                "task_id": task_id,
                "start_offset": start_offset,
                "read_bytes": end_offset - start_offset
            }
            r.rpush('queue:indexing:mapper', json.dumps(task))
            task_id += 1

    print(f"ğŸš€ Published {task_id} Mapper tasks to 'queue:indexing:mapper'")


def publish_reducer_tasks(r, num_reducers):  # è™½ç„¶å‚æ•°åè¿˜æ²¡æ”¹ï¼Œä½†é€»è¾‘å¦‚ä¸‹
    # å¼ºåˆ¶è¦†ç›–ä¸º 16ï¼Œæˆ–è€…åœ¨è°ƒç”¨æ—¶ä¼ å…¥ 16
    REAL_PARTITIONS = 16
    print(f"âš™ï¸  Publishing {REAL_PARTITIONS} Partition tasks...")

    for i in range(REAL_PARTITIONS):
        # ç®€å•çš„å‘æ•°å­— ID å³å¯ï¼ŒReducer ä¼šå…¼å®¹å¤„ç†
        r.rpush('queue:indexing:reducer', str(i))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--phase", choices=["map", "reduce", "all"], default="all")
    parser.add_argument("--reducers", type=int, default=4)
    parser.add_argument("--chunk_size", type=int, default=2000)
    args = parser.parse_args()

    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

    if args.phase in ["map", "all"]:
        reset_redis(r)  # åªæœ‰ map é˜¶æ®µæ‰æ¸…ç©ºï¼Œå› ä¸º reduce ä¾èµ– map çš„ç»“æœ
        publish_mapper_tasks(r, args.chunk_size)

    if args.phase in ["reduce", "all"]:
        # æ³¨æ„ï¼šå®é™…ä¸Šé€šå¸¸ç­‰ Map å®Œäº†å†å‘ Reduce ä»»åŠ¡ï¼Œè¿™é‡Œä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ä¸€èµ·å‘
        # æˆ–è€…ä½ å¯ä»¥åˆ†ä¸¤æ¬¡è¿è¡Œè„šæœ¬
        if args.phase == "reduce":
            # å¦‚æœåªå‘ reduce ä»»åŠ¡ï¼Œæ¸…ç†ä¸€ä¸‹æ—§çš„ reduce é˜Ÿåˆ—
            r.delete('queue:indexing:reducer')
        publish_reducer_tasks(r, args.reducers)