import os
import pickle
import glob
import heapq
import sys
from itertools import groupby
from psycopg2.extras import Json  # ç”¨äºå¤„ç† JSONB

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from compute.db_utils import get_db_connection

NUM_PARTITIONS = 16
DATA_DIR = "/app/data"
TEMP_DIR = os.path.join(DATA_DIR, "temp_shuffle")


def run_reducer_task(partition_id):
    print(f"âš™ï¸  [Reducer] Processing Partition {partition_id}...")

    pattern = os.path.join(TEMP_DIR, f"part-task*-r{partition_id}.pkl")
    files = glob.glob(pattern)

    if not files:
        print(f"   âš ï¸ No files, skipping.")
        return

    conn = None
    cursor = None
    file_handles = []

    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        iterators = []
        for fname in files:
            f = open(fname, 'rb')
            file_handles.append(f)
            iterators.append(pickle.load(f))

        merged_stream = heapq.merge(*iterators, key=lambda x: x[0])

        # Postgres Upsert
        sql = """
            INSERT INTO inverted_index (term, df, postings)
            VALUES (%s, %s, %s)
            ON CONFLICT (term) DO UPDATE 
            SET df = EXCLUDED.df, postings = EXCLUDED.postings;
        """

        batch_data = []
        BATCH_SIZE = 3000  # JSONB æ•°æ®é‡å¤§ï¼ŒBatch ç¨å¾®è°ƒå°
        count_terms = 0

        # æµå…ƒç´ : (term, doc_id, tf)
        for term, group in groupby(merged_stream, key=lambda x: x[0]):

            # è¿‡æ»¤è¶…é•¿åƒåœ¾è¯
            if len(term.encode('utf-8')) > 512: continue

            # === èšåˆé€»è¾‘ ===
            postings_map = {}
            for _, doc_id, tf in group:
                # ç´¯åŠ  TF (æ­£å¸¸æƒ…å†µä¸‹æ¯ä¸ª doc_id åªå‡ºç°ä¸€æ¬¡ï¼Œä½†ä¸ºäº†å¥å£®æ€§åšç´¯åŠ )
                postings_map[doc_id] = postings_map.get(doc_id, 0) + tf

            df = len(postings_map)

            # å­˜å…¥ Json å¯¹è±¡
            batch_data.append((term, df, Json(postings_map)))
            count_terms += 1

            if len(batch_data) >= BATCH_SIZE:
                cursor.executemany(sql, batch_data)
                batch_data = []
                print(f"   Indexed {count_terms} terms...", end='\r')

        if batch_data:
            cursor.executemany(sql, batch_data)

        conn.commit()
        print(f"\nâœ… [Reducer] Partition {partition_id} Done. ({count_terms} terms)")

    except Exception as e:
        if conn: conn.rollback()
        raise e
    finally:
        for f in file_handles: f.close()
        if cursor: cursor.close()
        if conn: conn.close()


# å•æœºå¾ªç¯è¿è¡Œæ‰€æœ‰åˆ†åŒº
def run_all_reducers():
    print("ğŸš€ Starting Reducer Sequence (JSONB Mode)...")
    for i in range(NUM_PARTITIONS):
        try:
            run_reducer_task(i)
        except Exception as e:
            print(f"âŒ Partition {i} Failed: {e}")


if __name__ == "__main__":
    run_all_reducers()