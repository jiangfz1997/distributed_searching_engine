import os
import pickle
import glob
import heapq
import sys
import json
import redis
import time
from itertools import groupby
from psycopg2.extras import Json

# å¼•å…¥ db_utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from compute.db_utils import get_db_connection

# === é…ç½® ===
NUM_PARTITIONS = 16
DATA_DIR = "/app/data"
TEMP_DIR = os.path.join(DATA_DIR, "temp_shuffle")

# === Redis é…ç½® ===
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
Q_SOURCE = 'queue:indexing:reducer'
Q_PROCESSING = 'queue:indexing:reducer:processing'
Q_DEAD = 'queue:indexing:reducer:dead'


def run_reducer_task(partition_id):
    """
    æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼šå¤„ç†æŒ‡å®šçš„åˆ†åŒºæ–‡ä»¶ï¼Œå…¥åº“ Postgres
    """
    print(f"âš™ï¸  [Reducer] Processing Partition {partition_id}...", flush=True)

    pattern = os.path.join(TEMP_DIR, f"part-task*-r{partition_id}.pkl")
    files = glob.glob(pattern)

    if not files:
        print(f"   âš ï¸ No files found for partition {partition_id}, skipping.", flush=True)
        return

    conn = None
    cursor = None
    file_handles = []

    try:
        conn = get_db_connection()
        cursor = conn.cursor()

        iterators = []
        for fname in files:
            f = open(fname, 'rb')
            file_handles.append(f)
            iterators.append(pickle.load(f))

        # å¤šè·¯å½’å¹¶
        merged_stream = heapq.merge(*iterators, key=lambda x: x[0])

        # Postgres JSONB Upsert
        sql = """
            INSERT INTO inverted_index (term, df, postings)
            VALUES (%s, %s, %s)
            ON CONFLICT (term) DO UPDATE 
            SET df = EXCLUDED.df, postings = EXCLUDED.postings;
        """
        doc_length_map = {}

        batch_data = []
        BATCH_SIZE = 3000
        count_terms = 0

        for term, group in groupby(merged_stream, key=lambda x: x[0]):
            if len(term.encode('utf-8')) > 512: continue

            # èšåˆé€»è¾‘
            postings_map = {}
            for _, doc_id, tf in group:
                postings_map[doc_id] = postings_map.get(doc_id, 0) + tf
                doc_length_map[doc_id] = doc_length_map.get(doc_id, 0) + tf

            df = len(postings_map)

            # ä½¿ç”¨ Json åŒ…è£…å™¨
            batch_data.append((term, df, Json(postings_map)))
            count_terms += 1

            if len(batch_data) >= BATCH_SIZE:
                cursor.executemany(sql, batch_data)
                batch_data = []

        if batch_data:
            cursor.executemany(sql, batch_data)

        doc_sql = """
                    INSERT INTO metadata (doc_id, length)
                    VALUES (%s, %s)
                    ON CONFLICT (doc_id) DO UPDATE
                    SET length = COALESCE(metadata.length, 0) + EXCLUDED.length;
                """

        doc_batch = [(doc_id, length) for doc_id, length in doc_length_map.items()]
        cursor.executemany(doc_sql, doc_batch)

        conn.commit()
        print(f"âœ… [Reducer] Partition {partition_id} Done. ({count_terms} terms)", flush=True)

    except Exception as e:
        if conn: conn.rollback()
        raise e  # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚ Worker å¤„ç†é‡è¯•é€»è¾‘
    finally:
        for f in file_handles: f.close()
        if cursor: cursor.close()
        if conn: conn.close()


def handle_error(r, raw_task, partition_id, error_msg, retries):
    """å¼‚å¸¸å¤„ç†ï¼šé‡è¯•æœºåˆ¶"""
    pipe = r.pipeline()
    pipe.lrem(Q_PROCESSING, 1, raw_task)  # å…ˆç§»é™¤å½“å‰å¤„ç†ä¸­çš„

    if retries < 3:
        print(f"âš ï¸ [Reducer] Partition {partition_id} failed ({retries + 1}/3). Retrying...", flush=True)
        # é‡æ–°æ‰“åŒ…ï¼Œå¢åŠ  retry è®¡æ•°
        new_task_data = {"id": partition_id, "retries": retries + 1}
        # å¡å›æºé˜Ÿåˆ—å¤´éƒ¨ï¼Œä¼˜å…ˆé‡è¯•
        pipe.lpush(Q_SOURCE, json.dumps(new_task_data))
    else:
        print(f"ğŸ’€ [Reducer] Partition {partition_id} DIED. Reason: {error_msg}", flush=True)
        dead_msg = {"id": partition_id, "error": str(error_msg)}
        pipe.rpush(Q_DEAD, json.dumps(dead_msg))

    pipe.execute()


def run_worker():
    """
    åˆ†å¸ƒå¼ Worker ä¸»å¾ªç¯
    """
    print(f"ğŸ”Œ Connecting to Redis at {REDIS_HOST}...", flush=True)
    try:
        r = redis.Redis(host=REDIS_HOST, port=6379, decode_responses=True)
    except Exception as e:
        print(f"âŒ Redis connection failed: {e}", flush=True)
        return

    print("ğŸ‘· Reducer Worker Started. Waiting for partitions...", flush=True)

    # ç©ºé—²é€€å‡ºæœºåˆ¶
    MAX_IDLE = 5
    idle_count = 0

    while True:
        # å¯é é˜Ÿåˆ—æ¨¡å¼ï¼šä» Source å¼¹å‡ºå¹¶æ¨å…¥ Processing
        raw_task = r.brpoplpush(Q_SOURCE, Q_PROCESSING, timeout=2)

        if not raw_task:
            idle_count += 1
            if idle_count >= MAX_IDLE:
                print("ğŸ‘‹ Queue empty. Reducer exiting.", flush=True)
                break
            continue

        idle_count = 0  # é‡ç½®ç©ºé—²è®¡æ•°

        partition_id = None
        retries = 0

        try:
            # è§£æä»»åŠ¡ï¼šå…¼å®¹çº¯æ•°å­— "0" å’Œ JSON '{"id":0, "retries":1}'
            try:
                task_dict = json.loads(raw_task)
                if isinstance(task_dict, int):
                    partition_id = task_dict
                else:
                    partition_id = task_dict['id']
                    retries = task_dict.get('retries', 0)
            except:
                partition_id = int(raw_task)

            # === æ‰§è¡Œæ ¸å¿ƒä»»åŠ¡ ===
            run_reducer_task(partition_id)

            # === æˆåŠŸï¼šACK (ä» Processing ç§»é™¤) ===
            r.lrem(Q_PROCESSING, 1, raw_task)

        except Exception as e:
            print(f"âŒ Worker Error processing {raw_task}: {e}", flush=True)
            # å¤±è´¥å¤„ç†ï¼šé‡è¯•æˆ–æ­»ä¿¡
            if partition_id is not None:
                handle_error(r, raw_task, partition_id, str(e), retries)
            else:
                # æ— æ³•è§£æçš„ä»»åŠ¡ç›´æ¥ç§»é™¤ï¼Œé˜²æ­¢æ­»å¾ªç¯
                r.lrem(Q_PROCESSING, 1, raw_task)


if __name__ == "__main__":
    run_worker()