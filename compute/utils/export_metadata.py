import json
import os
import sys
from tqdm import tqdm

# 1. Ë∑ØÂæÑËÆæÁΩÆÔºöÁ°Æ‰øùËÉΩÂºïÁî®Âà∞ compute Ê®°Âùó
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from compute.db_utils import get_db_connection
# 2. ÂºïÂÖ•ÂÖ±‰∫´ÂàÜËØçÂô® (NLTK Analyzer)
from compute.utils.tokenizer import analyzer

# === ÈÖçÁΩÆ ===
DATA_DIR = "/app/data"
INPUT_FILE = os.path.join(DATA_DIR, "intermediate", "corpus.jsonl")


def clean_text(text):
    """
    Ê∏ÖÊ¥ó Postgres ‰∏çÊîØÊåÅÁöÑÂ≠óÁ¨¶ (NUL Byte)
    """
    if not text:
        return ""
    return text.replace('\x00', '')


def export_metadata():
    print(f"üîå Connecting to PostgreSQL...")
    try:
        conn = get_db_connection()
        cur = conn.cursor()
    except Exception as e:
        print(f"‚ùå Database connection failed: {e}")
        return

    # 1. Ê∏ÖÁêÜÊóßÊï∞ÊçÆ
    print("üßπ Truncating 'metadata' table...")
    try:
        cur.execute("TRUNCATE TABLE metadata;")
        conn.commit()
    except Exception as e:
        print(f"‚ö†Ô∏è Truncate warning: {e}")
        conn.rollback()

    print(f"üöÄ Extracting metadata from {INPUT_FILE} (using NLTK Analyzer)...")

    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå Input file not found: {INPUT_FILE}")
        return

    batch_data = []
    BATCH_SIZE = 2000

    total_length = 0
    doc_count = 0

    insert_sql = """
        INSERT INTO metadata (doc_id, length, text) 
        VALUES (%s, %s, %s)
        ON CONFLICT (doc_id) DO UPDATE 
        SET length = EXCLUDED.length, text = EXCLUDED.text;
    """

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        # ‰ΩøÁî® tqdm ÊòæÁ§∫ËøõÂ∫¶
        for line in tqdm(f, desc="Processing & Tokenizing"):
            try:
                doc = json.loads(line)
                doc_id = doc['id']

                # Ê∏ÖÊ¥óÊñáÊú¨ (Áî®‰∫éÂ≠òÂÇ®ÂíåÂ±ïÁ§∫ Snippet)
                raw_text = doc.get('text', "")
                clean_content = clean_text(raw_text)

                # === Ê†∏ÂøÉ‰øÆÊîπÔºö‰ΩøÁî® NLTK Analyzer ËÆ°ÁÆó‚ÄúÊúâÊïàÈïøÂ∫¶‚Äù ===
                # ËøôÈáåÁöÑ length ‰∏çÂÜçÊòØÂçïËØçÊï∞ÔºåËÄåÊòØ‚ÄúÂéªÂÅúÁî®ËØçÂêé„ÄÅËØçÂπ≤ÊèêÂèñÂêéÁöÑÊúâÊïàËØçÊ†πÊï∞‚Äù
                # Ëøô‰∏é Indexing Èò∂ÊÆµÂÆåÂÖ®ÂØπÈΩêÔºå‰øùËØÅ BM25 ËÆ°ÁÆóÁöÑÁßëÂ≠¶ÊÄß
                tokens = analyzer.analyze(clean_content)
                length = len(tokens)

                # Êî∂ÈõÜÁªüËÆ°‰ø°ÊÅØ
                total_length += length
                doc_count += 1

                batch_data.append((doc_id, length, clean_content))

            except json.JSONDecodeError:
                continue
            except Exception as e:
                # print(f"‚ö†Ô∏è Error: {e}")
                continue

            # ÊâπÈáèÂÜôÂÖ•
            if len(batch_data) >= BATCH_SIZE:
                try:
                    cur.executemany(insert_sql, batch_data)
                    conn.commit()
                except Exception as e:
                    conn.rollback()
                    print(f"‚ö†Ô∏è Batch insert failed: {e}")
                batch_data = []

    # ÂÜôÂÖ•Ââ©‰Ωô
    if batch_data:
        try:
            cur.executemany(insert_sql, batch_data)
            conn.commit()
        except Exception as e:
            conn.rollback()
            print(f"‚ö†Ô∏è Final batch failed: {e}")

    # 2. ËÆ°ÁÆóÂπ∂Â≠òÂÇ®ÂÖ®Â±ÄÁªüËÆ°Èáè (AvgDL)
    avg_dl = total_length / doc_count if doc_count > 0 else 0.0
    print(f"üìä Statistics: Total Docs={doc_count}, AvgDL={avg_dl:.2f}")

    # Â≠òÂÖ• config Ë°®
    try:
        cur.execute("""
            INSERT INTO config (key, value) VALUES (%s, %s)
            ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value;
        """, ('avgdl', avg_dl))
        conn.commit()
    except Exception as e:
        conn.rollback()
        print(f"‚ö†Ô∏è Config update failed: {e}")

    cur.close()
    conn.close()
    print("‚úÖ Metadata export complete (NLTK Consistent)!")


if __name__ == "__main__":
    export_metadata()