import redis
import json
import time
import os
import random
# === é…ç½® ===
REDIS_HOST = "redis"
DAMPING_FACTOR = 0.85


def retry_execute(pipe, max_retries=3, backoff=1):
    """
    å°è¯•æ‰§è¡Œ Pipelineï¼Œå¦‚æžœé‡åˆ°è¿žæŽ¥é”™è¯¯æˆ–è¶…æ—¶åˆ™é‡è¯•ã€‚
    """
    for attempt in range(max_retries):
        try:
            return pipe.execute()
        except (redis.ConnectionError, redis.TimeoutError) as e:
            if attempt == max_retries - 1:
                print(f"âŒ Pipeline failed after {max_retries} attempts: {e}")
                raise e  # æŠ›å‡ºå¼‚å¸¸ï¼Œè®© Worker å´©æºƒ/é‡å¯ï¼Œç»å¯¹ä¸èƒ½åžæŽ‰å¼‚å¸¸ï¼

            sleep_time = backoff * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 1s, 2s, 4s
            print(f"âš ï¸ Pipeline write failed ({e}), retrying in {sleep_time}s...")
            time.sleep(sleep_time)
            # æ³¨æ„ï¼šPipeline å¯¹è±¡åœ¨ execute å¤±è´¥åŽé€šå¸¸ä¿æŒåŽŸæ ·ï¼Œå¯ä»¥ç›´æŽ¥å†æ¬¡ execute
    return None


def run_worker():
    r = redis.Redis(host=REDIS_HOST, port=6379, decode_responses=True)
    worker_pid = os.getpid()
    print(f"ðŸ‘· Worker {worker_pid} Ready. Waiting for signals...")
    start_delay = random.uniform(0, 2)
    time.sleep(start_delay)
    while True:
        # 1. èŽ·å–å½“å‰ä¿¡å·
        signal = r.get("sys:signal")

        if signal == "SHUTDOWN":
            print("ðŸ‘‹ Shutdown signal received.")
            break

        if signal not in ["SCATTER", "COMPUTE"]:
            # Controller è¿˜æ²¡å‡†å¤‡å¥½
            time.sleep(0.2)
            continue

        # 2. æŠ¢ä»»åŠ¡ (Micro-batch)
        # LPOP: éžé˜»å¡žå¼¹å‡ºã€‚å¦‚æžœæƒ³æ›´å®‰å…¨å¯ç”¨ BLPOP æˆ– RPOPLPUSH
        raw_task = r.lpop("queue:pr:tasks")

        if not raw_task:
            # æ²¡ä»»åŠ¡äº†ï¼Œä¼‘æ¯ç­‰å¾…ä¸‹ä¸€é˜¶æ®µ
            time.sleep(0.1)
            continue

        # 3. è§£æžä»»åŠ¡ "start,count"
        try:
            start_idx, count = map(int, raw_task.split(','))
            end_idx = start_idx + count - 1

            # èŽ·å–å…·ä½“çš„èŠ‚ç‚¹ ID åˆ—è¡¨
            node_ids = r.lrange("graph:nodes", start_idx, end_idx)
            if not node_ids: continue

            # === æ ¹æ®ä¿¡å·æ‰§è¡Œä¸åŒé€»è¾‘ ===

            if signal == "SCATTER":
                do_scatter(r, node_ids)

            elif signal == "COMPUTE":
                do_compute(r, node_ids)
            r.incr("sys:phase_ack")
        except Exception as e:
            print(f"âŒ Error processing task {raw_task}: {e}")

            # ðŸ”¥ðŸ”¥ðŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæŠŠä»»åŠ¡å¡žå›žé˜Ÿåˆ—å¤´ï¼Œè®©åˆ«äººï¼ˆæˆ–è€…è‡ªå·±ç­‰ä¼šå„¿ï¼‰å†åšä¸€æ¬¡
            print(f"â™»ï¸ Retrying task {raw_task}...")
            r.lpush("queue:pr:tasks", raw_task)

            # ç¨å¾®ç¡ä¸€ä¸‹ï¼Œé¿å¼€å½“å‰çš„æ•…éšœé£Žå¤´
            time.sleep(1)
            # ç”Ÿäº§çŽ¯å¢ƒåº”å°†ä»»åŠ¡å¡žå›žé˜Ÿåˆ—


def do_scatter(r, nodes):
    """
    Phase 1: è¯»å– Current PR -> åˆ†å‘ç»™é‚»å±… (ç´¯åŠ ) -> ç»Ÿè®¡æ‚¬æŒ‚èŠ‚ç‚¹
    """
    print(" -> Phase 1: Scatter Nodes")
    pipe = r.pipeline()

    # æ‰¹é‡èŽ·å–å½“å‰åˆ†æ•°å’Œå‡ºé“¾ä¿¡æ¯
    # æŠ€å·§: ä¸ºäº†å‡å°‘ IOï¼Œæˆ‘ä»¬å‡è®¾å‡ºé“¾åœ¨ graph:out_linksï¼Œåˆ†æ•°åœ¨ pr:ranks:current
    # ç”±äºŽ pipeline åªèƒ½æŒ‰é¡ºåºè¿”å›žï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸€å¯¹åº”

    for node in nodes:
        pipe.hget("pr:ranks:current", node)
        pipe.hget("graph:out_links", node)

    results = pipe.execute()

    # å‡†å¤‡å†™å…¥ç®¡é“
    write_pipe = r.pipeline()
    dangling_sum_local = 0.0

    # results æ˜¯ [score1, links1, score2, links2 ...]
    for i in range(0, len(results), 2):
        score_str = results[i]
        links_str = results[i + 1]

        current_score = float(score_str) if score_str else 0.0

        if not links_str:
            # === æ‚¬æŒ‚èŠ‚ç‚¹ ===
            # æ²¡æœ‰å‡ºé“¾ï¼Œåˆ†æ•°è´¡çŒ®ç»™å…¨å±€ dangling_sum
            dangling_sum_local += current_score
        else:
            # === æ­£å¸¸èŠ‚ç‚¹ ===
            targets = json.loads(links_str)
            out_degree = len(targets)
            if out_degree > 0:
                contribution = current_score / out_degree
                for target in targets:
                    # ä½¿ç”¨ HINCRBYFLOAT åŽŸå­ç´¯åŠ 
                    write_pipe.hincrbyfloat("pr:accumulated", target, contribution)

    # æäº¤ç´¯åŠ å€¼
    if dangling_sum_local > 0:
        write_pipe.hincrbyfloat("pr:dangling_sum", "total", dangling_sum_local)

    retry_execute(write_pipe)
    print(f"Scatter done for nodes. Dangling Sum Local: {dangling_sum_local}")

def do_compute(r, nodes):
    """Phase 2: è®¡ç®—æ–°åˆ†æ•° + è®¡ç®—æ”¶æ•›è¯¯å·®"""
    print(" -> Phase 2: Compute Nodes")
    base_val = float(r.get("sys:base_value") or 0.0)

    pipe = r.pipeline()
    for node in nodes:
        pipe.hget("pr:accumulated", node)  # èŽ·å–åˆ«äººç»™æˆ‘çš„æ€»é’±æ•°
        pipe.hget("pr:ranks:current", node)  # èŽ·å–æˆ‘ä¸Šä¸€è½®çš„æ—§åˆ†æ•° (ç”¨äºŽå¯¹æ¯”)
    results = pipe.execute()

    write_pipe = r.pipeline()
    local_diff_sum = 0.0

    for i, node in enumerate(nodes):
        # ç´¢å¼• i*2 æ˜¯ accumulated, i*2+1 æ˜¯ old_score
        accum_val = float(results[i * 2] or 0.0)
        old_score = float(results[i * 2 + 1] or 0.0)

        # æ ¸å¿ƒå…¬å¼
        new_score = base_val + (DAMPING_FACTOR * accum_val)

        # è®°å½•æ–°åˆ†æ•°
        write_pipe.hset("pr:ranks:next", node, new_score)

        # è®¡ç®—è¯¯å·® diff
        local_diff_sum += abs(new_score - old_score)

    # æäº¤æ–°åˆ†æ•°
    retry_execute(write_pipe)

    # æäº¤è¯¯å·®ç»Ÿè®¡ (ç”¨äºŽ Controller åˆ¤æ–­æ˜¯å¦æå‰æ”¶æ•›)
    if local_diff_sum > 0:
        r.incrbyfloat("sys:convergence_diff", local_diff_sum)
    print(f"Compute done for nodes. Local Diff Sum: {local_diff_sum}")

if __name__ == "__main__":
    run_worker()