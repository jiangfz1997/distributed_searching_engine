import redis
import time
import math
import sys,os,csv

# === é…ç½® ===
REDIS_HOST = "redis"
TASK_BATCH_SIZE = 2000  # æ¯ä¸ªä»»åŠ¡åŒ…åŒ…å«å¤šå°‘ä¸ªèŠ‚ç‚¹
MAX_ITERATIONS = 100  # è¿­ä»£æ¬¡æ•°
DAMPING_FACTOR = 0.85
CONVERGENCE_THRESHOLD = 1e-06 # æ”¶æ•›é˜ˆå€¼ (æ€»è¯¯å·®å°äºæ­¤å€¼å³åœæ­¢)
LOG_FILE = "/app/log/output/pr_convergence.csv"

def generate_tasks(r, total_nodes):
    """ç”Ÿæˆä»»åŠ¡åŒ…ï¼šåˆ†æ‰¹å†™å…¥ Redis å¹¶æ˜¾ç¤ºè¿›åº¦"""

    # 1. æ¸…ç†æ—§é˜Ÿåˆ—
    if r.exists("queue:pr:tasks"):
        # print("ğŸ§¹ Clearing old task queue...")
        r.delete("queue:pr:tasks")

    pipe = r.pipeline()
    task_count = 0

    # 2. è®¡ç®—æ€»ä»»åŠ¡æ•°ï¼ˆç”¨äºæ˜¾ç¤ºç™¾åˆ†æ¯”ï¼‰
    total_tasks = math.ceil(total_nodes / TASK_BATCH_SIZE)

    # 3. åˆ†æ‰¹ç”Ÿæˆ
    # PIPELINE_CHUNK: æ¯ç§¯æ”’å¤šå°‘ä¸ªä»»åŠ¡æäº¤ä¸€æ¬¡ Redis (é˜²æ­¢å†…å­˜ç§¯å‹)
    PIPELINE_CHUNK = 1000

    print(f"ğŸ“¦ Generating {total_tasks} tasks (Batch Size: {TASK_BATCH_SIZE})...")

    for start in range(0, total_nodes, TASK_BATCH_SIZE):
        # ä»»åŠ¡æ ¼å¼: "start_index,count"
        pipe.rpush("queue:pr:tasks", f"{start},{TASK_BATCH_SIZE}")
        task_count += 1

        # æ¯ç§¯ç´¯ 1000 ä¸ªä»»åŠ¡ï¼Œæˆ–è€…è¾¾åˆ°æ€»æ•°ï¼Œå°±æäº¤ä¸€æ¬¡
        if task_count % PIPELINE_CHUNK == 0:
            pipe.execute()  # çœŸæ­£å†™å…¥ Redis
            # æ‰“å°è¿›åº¦
            percent = (task_count / total_tasks) * 100
            print(f"    - Generated {task_count}/{total_tasks} tasks ({percent:.1f}%)", end='\r')

    # æäº¤å‰©ä½™çš„ä»»åŠ¡
    pipe.execute()
    print(f"âœ… Generated {task_count} tasks in total.        ")  # ç©ºæ ¼æ˜¯ä¸ºäº†è¦†ç›–ä¸Šé¢çš„ \r

    return task_count


def wait_for_tasks(r, total_tasks):
    """
    ç­‰å¾…æ‰€æœ‰ä»»åŠ¡è¢«ã€å®Œæˆã€‘(ACK)ï¼Œè€Œä¸ä»…ä»…æ˜¯è¢«ã€é¢†èµ°ã€‘
    """
    print(f"    Waiting for {total_tasks} tasks to complete...", end='', flush=True)

    while True:
        # è·å–å·²å®Œæˆçš„ä»»åŠ¡æ•° (ACK)
        # è¿™é‡Œçš„ key æ˜¯ sys:phase_ack
        done_count = int(r.get("sys:phase_ack") or 0)

        # æ‰“å°è¿›åº¦æ¡æ•ˆæœ
        percent = (done_count / total_tasks) * 100
        print(f"\r    Waiting for {total_tasks} tasks... {percent:.1f}% ({done_count}/{total_tasks})", end='',
              flush=True)

        if done_count >= total_tasks:
            print("")  # æ¢è¡Œ
            return

        time.sleep(0.2)  # è½®è¯¢é—´éš”


def run_controller():
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    print(f"ğŸ“ Logging convergence data to {LOG_FILE}...")
    print(f"CONVERGENCE_THRESHOLD = {CONVERGENCE_THRESHOLD}")
    r = redis.Redis(host=REDIS_HOST, port=6379, decode_responses=True)
    cleanup_state(r)
    # æ£€æŸ¥å›¾æ˜¯å¦åŠ è½½
    if not r.exists("sys:node_count"):
        print("âŒ Graph not found! Run graph_loader.py first.")
        sys.exit(1)

    total_nodes = int(r.get("sys:node_count"))
    print(f"ğŸš¦ Controller Started. Nodes: {total_nodes}")

    with open(LOG_FILE, mode='w', newline='') as f:
        writer = csv.writer(f)
        # å†™å…¥è¡¨å¤´
        writer.writerow(['Round', 'Duration_Seconds', 'Diff_Value'])

    for round_id in range(1, MAX_ITERATIONS + 1):
        print(f"\n=== ğŸ ROUND {round_id} START ===")
        start_time = time.time()

        # ==========================================
        # PHASE 1: SCATTER (åˆ†å‘è´¡çŒ® + ç»Ÿè®¡æ‚¬æŒ‚)
        # ==========================================
        print(" -> Phase 1: Scatter")

        # 1. æ¸…ç†ä¸­é—´æ•°æ®
        r.delete("pr:accumulated")  # è¿™ä¸€è½®æ”¶åˆ°çš„ä¿¡ä»¶ç®±
        r.delete("pr:dangling_sum")  # æ‚¬æŒ‚èŠ‚ç‚¹æ€»å’Œ
        r.set("sys:phase_ack", 0)
        # 2. ç”Ÿæˆä»»åŠ¡
        num_tasks = generate_tasks(r, total_nodes)

        # 3. å‘é€ä¿¡å·
        r.set("sys:signal", "SCATTER")

        # 4. ç­‰å¾…å®Œæˆ
        wait_for_tasks(r, num_tasks)

        # ==========================================
        # ä¸­é—´è®¡ç®—: å‡†å¤‡ Base Value
        # ==========================================
        dangling_sum = float(r.hget("pr:dangling_sum", "total") or 0.0)
        # PageRank å…¬å¼:
        # PR(u) = (1-d)/N + d * (Sum_In_Links + Dangling_Sum / N)
        # æå–å…¬å› å¼ Base = (1-d + d * Dangling_Sum) / N
        base_value = (1.0 - DAMPING_FACTOR + (DAMPING_FACTOR * dangling_sum)) / total_nodes

        # å°† Base å­˜å…¥ Redis ä¾› Worker åœ¨ Phase 2 ä½¿ç”¨
        r.set("sys:base_value", base_value)
        print(f"    (Dangling Sum: {dangling_sum:.4f}, Base Value: {base_value:.8f})")

        # ==========================================
        # PHASE 2: COMPUTE (åº”ç”¨å…¬å¼ + å†™å…¥ç»“æœ)
        # ==========================================
        print(" -> Phase 2: Compute")
        r.set("sys:convergence_diff", 0.0)  # é‡ç½®æœ¬è½®è¯¯å·®è®¡æ•°å™¨
        # 1. æ¸…ç†ä¸‹ä¸€è½®ç»“æœè¡¨
        r.delete("pr:ranks:next")
        r.set("sys:phase_ack", 0)
        # 2. å†æ¬¡ç”ŸæˆåŒæ ·çš„ä»»åŠ¡ (è®© Worker éå†æ‰€æœ‰èŠ‚ç‚¹åº”ç”¨å…¬å¼)
        num_tasks = generate_tasks(r, total_nodes)

        # 3. å‘é€ä¿¡å·
        r.set("sys:signal", "COMPUTE")

        # 4. ç­‰å¾…å®Œæˆ
        wait_for_tasks(r, num_tasks)
        # ================= Check Convergence =================
        total_diff = float(r.get("sys:convergence_diff") or 0.0)
        duration = time.time() - start_time
        print(f"    Round {round_id} Done. Time: {duration:.2f}s, Diff: {total_diff:.6f}")
        with open(LOG_FILE, mode='a', newline='') as f:
            writer = csv.writer(f)
            # è¿™é‡Œçš„ diff å»ºè®®å­˜é«˜ç²¾åº¦
            writer.writerow([round_id, round(duration, 4), f"{total_diff:.10f}"])
        if total_diff < CONVERGENCE_THRESHOLD:
            print(f"âœ¨ Converged at Round {round_id}! (Diff {total_diff} < {CONVERGENCE_THRESHOLD})")
            break
        # ==========================================
        # SWAP (ç¿»è½¬)
        # ==========================================

        print(" -> Swapping current/next...")
        r.delete("pr:ranks:current")
        r.rename("pr:ranks:next", "pr:ranks:current")

        duration = time.time() - start_time
        print(f"âœ… Round {round_id} Done in {duration:.2f}s")

    print("\nğŸ‰ PageRank Completed.")
    r.set("sys:signal", "SHUTDOWN")


def cleanup_state(r):
    """
    å¯åŠ¨å‰æ¸…ç†ä¸Šä¸€è½®æ®‹ç•™çš„è¿è¡Œæ—¶ Keyï¼Œä½†ã€ä¸¥æ ¼ä¿ç•™ã€‘å›¾ç»“æ„æ•°æ®ã€‚
    """
    print("ğŸ§¹ Cleaning runtime state (keeping graph data)...")

    # è¿™äº›æ˜¯è¿è¡Œæ—¶çš„ä¸´æ—¶ Keyï¼Œåˆ äº†ä¸ä¼šå½±å“å›¾ç»“æ„
    keys_to_delete = [
        "queue:pr:tasks",  # ä»»åŠ¡é˜Ÿåˆ—
        "sys:signal",  # æ§åˆ¶ä¿¡å· (SCATTER/COMPUTE)
        "sys:phase_ack",  # é˜¶æ®µå®Œæˆè®¡æ•°å™¨
        "sys:base_value",  # PageRank åŸºç¡€å€¼
        "sys:convergence_diff",  # æ”¶æ•›è¯¯å·®
        "pr:accumulated",  # Scatter é˜¶æ®µçš„ç´¯åŠ æ± 
        "pr:dangling_sum",  # æ‚¬æŒ‚èŠ‚ç‚¹æ€»å’Œ
        "pr:ranks:next"  # ä¸‹ä¸€è½®åˆ†æ•°çš„ç¼“å†²åŒº
    ]

    r.delete(*keys_to_delete)

    # ã€å¯é€‰ã€‘å…³äº pr:ranks:current (å½“å‰åˆ†æ•°)
    # å¦‚æœä½ ä¿ç•™å®ƒï¼šPageRank ä¼šåŸºäºä¸Šä¸€æ¬¡çš„ç»“æœç»§ç»­ç®—ï¼ˆçƒ­å¯åŠ¨ï¼Œæ”¶æ•›æ›´å¿«ï¼‰ã€‚
    # å¦‚æœä½ åˆ æ‰å®ƒï¼šä½ éœ€è¦åœ¨è¿™é‡Œé‡æ–°åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ä¸º 1/Nã€‚
    #
    # ä¸ºäº†æ–¹ä¾¿è°ƒè¯•ï¼Œæˆ‘ä»¬è¿™é‡Œã€ä¿ç•™ã€‘å®ƒã€‚
    # å¦‚æœä½ æƒ³å¼ºåˆ¶é‡ç½®åˆ†æ•°ï¼Œå¯ä»¥åœ¨å‘½ä»¤è¡ŒåŠ ä¸ªå‚æ•°ï¼Œæˆ–è€…æ‰‹åŠ¨é‡ç½®ã€‚

    print("âœ¨ Runtime state cleared. Ready to start.")
if __name__ == "__main__":
    run_controller()