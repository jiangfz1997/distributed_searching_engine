import os
import sys
import json
import math
from collections import defaultdict

import numpy as np
from tqdm import tqdm

# æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´
sys.path.append("/app")

from serving.search_engine import SearchEngine


# ===== é…ç½® =====
QRELS_PATH = "/app/evaluation/manual_qrels.json"
TOPK = 20          # æ¯ä¸ª query ä»ç³»ç»Ÿé‡Œå–å¤šå°‘ä¸ªå€™é€‰
K_VALUES = [1, 3, 5, 10]  # è®¡ç®— @k çš„ä½ç½®ï¼Œå¯ä»¥æŒ‰éœ€æ”¹


def load_manual_qrels(path: str):
    """
    ä» JSON æ–‡ä»¶åŠ è½½æ‰‹åŠ¨æ ‡æ³¨çš„è¯„æµ‹æ•°æ®ã€‚

    æœŸæœ›æ ¼å¼:
    [
      {
        "query": "how photosynthesis works",
        "relevant": {
          "Photosynthesis": 3,
          "Chlorophyll": 2
        }
      },
      ...
    ]

    ä¹Ÿå…¼å®¹:
    "relevant": ["Photosynthesis", "Chlorophyll"]
    è¿™ç§å†™æ³•ä¼šè¢«è‡ªåŠ¨è½¬æ¢æˆäºŒå€¼ç›¸å…³æ€§ 1ã€‚
    """
    if not os.path.exists(path):
        print(f"âŒ Qrels file not found: {path}")
        print("   è¯·å…ˆåˆ›å»ºä¸€ä¸ª manual_qrels.json å¹¶å†™å…¥ä½ çš„æ‰‹åŠ¨æ ‡æ³¨ã€‚")
        sys.exit(1)

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    queries = []
    for item in data:
        query = item["query"]
        rel = item["relevant"]

        # æ”¯æŒ list æˆ– dict ä¸¤ç§å†™æ³•
        if isinstance(rel, list):
            rel_dict = {doc_id: 1 for doc_id in rel}
        else:
            rel_dict = dict(rel)

        queries.append((query, rel_dict))

    print(f"âœ… Loaded {len(queries)} manually labeled queries.")
    return queries


def ndcg_at_k(ranked_doc_ids, rel_dict, k):
    """
    è®¡ç®—å•ä¸ª query çš„ NDCG@kã€‚
    rel_dict: {doc_id: relevance_score}
    ranked_doc_ids: ç³»ç»Ÿè¿”å›çš„ doc_id åˆ—è¡¨
    """
    dcg = 0.0
    for rank, doc_id in enumerate(ranked_doc_ids[:k], start=1):
        rel = rel_dict.get(doc_id, 0)
        if rel > 0:
            dcg += rel / math.log2(rank + 1)

    # è®¡ç®—ç†æƒ³ DCGï¼ˆæŒ‰ç›¸å…³æ€§ä»å¤§åˆ°å°æ’åºï¼‰
    gains = sorted(rel_dict.values(), reverse=True)
    idcg = 0.0
    for rank, rel in enumerate(gains[:k], start=1):
        if rel > 0:
            idcg += rel / math.log2(rank + 1)

    if idcg == 0.0:
        return 0.0
    return dcg / idcg


def recall_at_k(ranked_doc_ids, rel_dict, k):
    """
    è®¡ç®—å•ä¸ª query çš„ Recall@kã€‚
    """
    if not rel_dict:
        return 0.0
    relevant_set = set(rel_dict.keys())
    retrieved_k = set(ranked_doc_ids[:k])
    hits = len(relevant_set & retrieved_k)
    return hits / len(relevant_set)


def run_manual_evaluation():
    # 1) åŠ è½½æ‰‹åŠ¨æ ‡æ³¨æ•°æ®
    eval_queries = load_manual_qrels(QRELS_PATH)

    # 2) åˆå§‹åŒ–æœç´¢å¼•æ“
    print("ğŸš€ Initializing Search Engine...")
    engine = SearchEngine()

    # 3) é€ä¸ª query æ‰§è¡Œæ£€ç´¢å¹¶è®°å½•ç»“æœ
    all_ndcg = {k: [] for k in K_VALUES}
    all_recall = {k: [] for k in K_VALUES}

    print(f"ğŸ” Running search for {len(eval_queries)} queries...")
    for query, rel_dict in tqdm(eval_queries, desc="Evaluating"):
        results = engine.search(query, topk=TOPK)
        ranked_doc_ids = [r["doc_id"] for r in results]

        for k in K_VALUES:
            ndcg = ndcg_at_k(ranked_doc_ids, rel_dict, k)
            rec = recall_at_k(ranked_doc_ids, rel_dict, k)
            all_ndcg[k].append(ndcg)
            all_recall[k].append(rec)

    # 4) æ±‡æ€»å¹¶æ‰“å°æ•´ä½“æŒ‡æ ‡
    print("\n" + "=" * 40)
    print("ğŸ† MANUAL EVALUATION REPORT")
    print(f"Queries Evaluated: {len(eval_queries)}")
    print("=" * 40)

    for k in K_VALUES:
        mean_ndcg = float(np.mean(all_ndcg[k])) if all_ndcg[k] else 0.0
        mean_recall = float(np.mean(all_recall[k])) if all_recall[k] else 0.0
        print(f"ğŸ‘‰ NDCG@{k:<2}: {mean_ndcg:.4f}")
        print(f"ğŸ‘‰ Recall@{k:<2}: {mean_recall:.4f}")
        print("-" * 20)

    print("=" * 40)
    print("âœ… Done.")


if __name__ == "__main__":
    run_manual_evaluation()
