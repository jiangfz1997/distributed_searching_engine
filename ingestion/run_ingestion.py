import os
import xml.etree.ElementTree as ET
import json
import mwparserfromhell  # <--- ç¥å™¨
from tqdm import tqdm

# === é…ç½® ===
# ç¡®ä¿è¿™é‡Œè·¯å¾„å¯¹å¾—ä¸Š
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
XML_FILE = os.path.join(BASE_DIR, "data/raw/simplewiki-latest-pages-articles.xml")
OUT_FILE = os.path.join(BASE_DIR, "data/intermediate/corpus.jsonl")


def normalize_id(title):
    if not title: return ""
    return title.strip().replace(" ", "_")


def process_wiki_dump():
    print(f"ğŸš€ Parsing XML: {XML_FILE}")
    if not os.path.exists(XML_FILE):
        print("âŒ XML file not found!")
        return

    context = ET.iterparse(XML_FILE, events=("end",))
    count = 0
    skipped = 0

    with open(OUT_FILE, "w", encoding="utf-8") as f_out:
        for event, elem in context:
            # å¤„ç† XML å‘½åç©ºé—´: {http://...}page -> page
            tag = elem.tag.split("}")[-1]

            if tag == "page":
                title = elem.find("{*}title").text if elem.find("{*}title") is not None else None
                revision = elem.find("{*}revision")
                text_node = revision.find("{*}text") if revision is not None else None
                raw_text = text_node.text if text_node is not None else ""

                # è¿‡æ»¤é‡å®šå‘å’Œéä¸»å‘½åç©ºé—´
                ns = elem.find("{*}ns")
                ns_val = int(ns.text) if ns is not None else 0

                # å¿…é¡»æ˜¯ä¸»æ¡ç›®(ns=0)ï¼Œä¸”ä¸æ˜¯ Redirect
                if title and raw_text and ns_val == 0 and not raw_text.lower().startswith("#redirect"):

                    try:
                        # === æ ¸å¿ƒé­”æ³•ï¼šä½¿ç”¨ mwparserfromhell è§£æ ===
                        wikicode = mwparserfromhell.parse(raw_text)

                        # 1. æå–çº¯æ–‡æœ¬ (è‡ªåŠ¨å»æ‰ {{...}}, <ref>, '''...''')
                        clean_text = wikicode.strip_code().strip()

                        # 2. æå–å‡ºé“¾ (PageRank éœ€è¦!)
                        # filter_wikilinks() ä¼šè‡ªåŠ¨æ‰¾åˆ° [[Target]]
                        links = []
                        for link in wikicode.filter_wikilinks():
                            # è·å–é“¾æ¥ç›®æ ‡ (e.g. "United States")
                            target = str(link.title)
                            # è¿‡æ»¤æ‰æ–‡ä»¶å’Œåˆ†ç±»é“¾æ¥
                            if ":" not in target:
                                links.append(normalize_id(target))

                        # å†™å…¥ç»“æœ
                        if len(clean_text) > 50:  # å¤ªçŸ­çš„ä¸¢æ‰
                            doc = {
                                "id": normalize_id(title),
                                "text": clean_text,
                                "out_links": links
                            }
                            f_out.write(json.dumps(doc) + "\n")
                            count += 1
                        else:
                            skipped += 1

                    except Exception as e:
                        print(f"âš ï¸ Error parsing {title}: {e}")
                        skipped += 1
                else:
                    skipped += 1

                # æ¸…ç†å†…å­˜
                elem.clear()
                if count % 1000 == 0:
                    print(f"âœ… Processed {count} docs...", end='\r')

    print(f"\nâœ¨ Done! Saved {count} docs to {OUT_FILE}")


if __name__ == "__main__":
    process_wiki_dump()